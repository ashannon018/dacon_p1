{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "train_df = pd.read_csv('./input/train.csv')\n",
    "test_df = pd.read_csv('./input/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming the target variable is named 'label'\n",
    "X_train = train_df.drop(columns=['SUBCLASS'])\n",
    "y_train = train_df['SUBCLASS']\n",
    "X_test = test_df\n",
    "y_test = test_df['SUBCLASS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_res_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Dimensionality Reduction using PCA\n",
    "pca = PCA(n_components=50)  # Adjust the number of components as needed\n",
    "X_train_pca = pca.fit_transform(X_train_res_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Step 4: Train LightGBM model\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "lgbm.fit(X_train_pca, y_train_res)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lgbm.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 5: Hyperparameter tuning (Optional)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'num_leaves': [31, 50]\n",
    "}\n",
    "grid_search = GridSearchCV(LGBMClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train_pca, y_train_res)\n",
    "\n",
    "# Best model evaluation\n",
    "best_lgbm = grid_search.best_estimator_\n",
    "y_pred_best = best_lgbm.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: Encode the 'SUBCLASS' column (target labels)\n",
    "le = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 2: Convert mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "mutation_cols = train_df.columns[2:]  # Exclude 'ID' and 'SUBCLASS'\n",
    "train_df[mutation_cols] = train_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 3: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "# Step 4: Dimensionality Reduction using PCA (Optional, but recommended for high-dimensional data)\n",
    "pca = PCA(n_components=100)  # Reduce to 100 components (tune this number as needed)\n",
    "X_pca = pca.fit_transform(train_df[mutation_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Prepare final dataset\n",
    "X = X_pca  # Features after PCA\n",
    "y = train_df['SUBCLASS']  # Target labels\n",
    "\n",
    "# Step 6: Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Build the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Input and first hidden layer\n",
    "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
    "model.add(Dense(64, activation='relu'))  # Second hidden layer\n",
    "model.add(Dropout(0.3))  # Dropout layer\n",
    "model.add(Dense(32, activation='relu'))  # Third hidden layer\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))  # Output layer (softmax for multiclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 8: Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 10: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 11: Print the classification report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=le.classes_))\n",
    "\n",
    "# Optional: Plot the training and validation accuracy and loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='validation loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the training dataset to get the mutation columns if necessary\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: Define the mutation columns (excluding 'ID' and 'SUBCLASS')\n",
    "mutation_cols = train_df.columns[2:]\n",
    "\n",
    "# Step 2: Load the test dataset\n",
    "test_path = './input/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Step 3: Encode the mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "test_df[mutation_cols] = test_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 4: Standardize the mutation features (using the same scaler fit on training data)\n",
    "test_df[mutation_cols] = scaler.transform(test_df[mutation_cols])\n",
    "\n",
    "# Step 5: Dimensionality Reduction using PCA (using the same PCA fit on training data)\n",
    "X_test_pca = pca.transform(test_df[mutation_cols])\n",
    "\n",
    "# Step 6: Predict using the trained model\n",
    "y_test_pred = model.predict(X_test_pca)\n",
    "y_test_pred_classes = y_test_pred.argmax(axis=1)\n",
    "\n",
    "# Step 7: Decode the predicted class labels back to their original subclass names\n",
    "y_test_pred_labels = le.inverse_transform(y_test_pred_classes)\n",
    "\n",
    "# Step 8: Create a DataFrame to store the predictions\n",
    "test_df['Predicted_SUBCLASS'] = y_test_pred_labels\n",
    "\n",
    "# Step 9: Display the first few rows of predictions\n",
    "test_df[['ID', 'Predicted_SUBCLASS']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['ID', 'Predicted_SUBCLASS']].to_csv('submit01.csv',index=False)\n",
    "test_df[['Predicted_SUBCLASS']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: Encode the 'SUBCLASS' column (target labels)\n",
    "le = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 2: Convert mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "mutation_cols = train_df.columns[2:]\n",
    "train_df[mutation_cols] = train_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 3: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "# Step 4: Feature Selection - Variance Threshold\n",
    "# Remove low variance features (those that do not vary much)\n",
    "threshold = 0.01  # Adjust threshold based on data characteristics\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "X_reduced = selector.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "print(f\"Reduced feature set after Variance Threshold: {X_reduced.shape[1]} features\")\n",
    "\n",
    "# Step 5: Train XGBoost to find important features\n",
    "X = X_reduced\n",
    "y = train_df['SUBCLASS']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "sorted_idx = feature_importances.argsort()\n",
    "\n",
    "# Select top 2000 features based on importance\n",
    "top_n = 2000  # Adjust this value to select the top N important features\n",
    "X_train_reduced = X_train[:, sorted_idx[-top_n:]]\n",
    "X_test_reduced = X_test[:, sorted_idx[-top_n:]]\n",
    "\n",
    "# Step 6: Train a model on the reduced feature set (after selecting important features)\n",
    "model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "y_pred = model.predict(X_test_reduced)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the reduced feature set: {accuracy}\")\n",
    "\n",
    "# Optional: Get feature importances\n",
    "important_features = sorted_idx[-top_n:]\n",
    "print(f\"Selected Top {top_n} features: {important_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './input/train_sample.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_sample_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./input/train_sample.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sample_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Step 1: Encode the 'SUBCLASS' column (target labels)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './input/train_sample.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: Encode the 'SUBCLASS' column (target labels)\n",
    "le = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 2: Convert mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "mutation_cols = train_df.columns[2:]\n",
    "train_df[mutation_cols] = train_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 3: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X = train_df[mutation_cols]\n",
    "y = train_df['SUBCLASS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train an XGBoost model to find important features\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "sorted_idx = feature_importances.argsort()\n",
    "\n",
    "# Select top 2000 features based on importance\n",
    "top_n = 2000  # Adjust this value to select the top N important features\n",
    "top_features_idx = sorted_idx[-top_n:]\n",
    "\n",
    "# Step 6: Reduce training and testing data to the top N features\n",
    "X_train_reduced = X_train.iloc[:, top_features_idx]\n",
    "X_test_reduced = X_test.iloc[:, top_features_idx]\n",
    "\n",
    "# Step 7: Train a new XGBoost model on the reduced feature set\n",
    "best_xgb = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "best_xgb.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Step 8: Evaluate the model on the test set\n",
    "y_pred = best_xgb.predict(X_test_reduced)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the reduced feature set: {accuracy}\")\n",
    "\n",
    "# Step 9: Load test.csv for prediction\n",
    "test_path = './input/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Preprocess the test data (same as train data preprocessing)\n",
    "test_df[mutation_cols] = test_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "test_df[mutation_cols] = scaler.transform(test_df[mutation_cols])\n",
    "\n",
    "# Reduce test data to the top N important features\n",
    "X_test_final = test_df[mutation_cols].iloc[:, top_features_idx]\n",
    "\n",
    "# Step 10: Predict using the trained model\n",
    "y_test_pred = best_xgb.predict(X_test_final)\n",
    "\n",
    "# Step 11: Decode the predicted class labels back to their original subclass names\n",
    "y_test_pred_labels = le.inverse_transform(y_test_pred)\n",
    "\n",
    "# Step 12: Store the predictions in the test DataFrame\n",
    "test_df['Predicted_SUBCLASS'] = y_test_pred_labels\n",
    "\n",
    "# Display the first few rows of predictions\n",
    "print(test_df[['ID', 'Predicted_SUBCLASS']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.239796172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['ID', 'Predicted_SUBCLASS']].to_csv('submit02.csv',index=False)\n",
    "test_df[['Predicted_SUBCLASS']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드 및 전처리 (이전 코드와 동일)\n",
    "train_sample_path = '/mnt/data/train_sample.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: Encode the 'SUBCLASS' column (target labels)\n",
    "le = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 2: Convert mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "mutation_cols = train_df.columns[2:]\n",
    "train_df[mutation_cols] = train_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 3: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X = train_df[mutation_cols]\n",
    "y = train_df['SUBCLASS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost 기본 모델 설정\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# GridSearchCV를 사용한 하이퍼파라미터 튜닝\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # 트리 개수\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # 학습률\n",
    "    'max_depth': [3, 5, 7],  # 트리의 최대 깊이\n",
    "    'min_child_weight': [1, 3, 5],  # 리프 노드에서 필요한 최소 가중치 합\n",
    "    'subsample': [0.6, 0.8, 1.0],  # 샘플링 비율\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]  # 각 트리에서 사용할 특징의 비율\n",
    "}\n",
    "\n",
    "# GridSearchCV를 사용해 최적의 하이퍼파라미터 탐색\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터와 정확도 출력\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델을 다시 학습\n",
    "best_xgb = grid_search.best_estimator_\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 테스트 세트에서 모델 평가\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드 및 전처리 (이전 코드와 동일)\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: 'ID' 컬럼 제거 (학습에 불필요한 열)\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "\n",
    "# Step 2: Encode the 'SUBCLASS' column (target labels)\n",
    "le = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 3: Convert mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "mutation_cols = train_df.columns[1:]  # 'SUBCLASS' 이후 모든 열이 변이 정보\n",
    "train_df[mutation_cols] = train_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 4: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "# Step 5: 상관계수 기반으로 특징 선택 (상관계수가 낮은 특징 제거)\n",
    "correlation_matrix = np.abs(train_df.corr())\n",
    "correlation_with_target = correlation_matrix['SUBCLASS'].drop('SUBCLASS')\n",
    "top_features = correlation_with_target[correlation_with_target > 0.1].index\n",
    "X = train_df[top_features]\n",
    "y = train_df['SUBCLASS']\n",
    "\n",
    "# Step 6: SMOTE 적용하여 클래스 불균형 해결\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Step 7: 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: XGBoost 모델 생성 (클래스 가중치 적용)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, scale_pos_weight=1.5, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: 모델 평가\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 10: Load test.csv for prediction\n",
    "test_path = './input/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Step 11: 'ID' 컬럼 제거 후 테스트 데이터 전처리\n",
    "test_df = test_df.drop(columns=['ID'])\n",
    "test_df[mutation_cols] = test_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "test_df[mutation_cols] = scaler.transform(test_df[mutation_cols])\n",
    "\n",
    "# Step 12: 테스트 데이터에서도 상위 중요 피처만 선택\n",
    "X_test_final = test_df[top_features]\n",
    "\n",
    "# Step 13: Predict using the trained model\n",
    "y_test_pred = xgb_model.predict(X_test_final)\n",
    "\n",
    "# Step 14: Decode the predicted class labels back to their original subclass names\n",
    "y_test_pred_labels = le.inverse_transform(y_test_pred)\n",
    "\n",
    "# Step 15: Store the predictions in the test DataFrame\n",
    "test_df['Predicted_SUBCLASS'] = y_test_pred_labels\n",
    "\n",
    "# Display the first few rows of predictions\n",
    "print(test_df[['Predicted_SUBCLASS']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['ID', 'Predicted_SUBCLASS']].to_csv('submit03.csv',index=False)\n",
    "test_df[['Predicted_SUBCLASS']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['Predicted_SUBCLASS']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['Predicted_SUBCLASS']].to_csv('submit03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_test = pd.read_csv(test_path)\n",
    "pd.concat(root_test['ID'],test_df[['Predicted_SUBCLASS']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original test dataset to retrieve 'ID' column\n",
    "root_test = pd.read_csv(test_path)\n",
    "\n",
    "# Ensure 'ID' is preserved as a DataFrame\n",
    "id_column = root_test[['ID']]\n",
    "\n",
    "# Concatenate the 'ID' column with the predicted results\n",
    "final_result = pd.concat([id_column, test_df[['Predicted_SUBCLASS']]], axis=1)\n",
    "\n",
    "# Display the first few rows of the final result\n",
    "print(final_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.to_csv('Submit03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: 'ID' 컬럼 제거 (학습에 불필요한 열)\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "\n",
    "# Step 2: Encode the 'SUBCLASS' column (target labels)\n",
    "le = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 3: Convert mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "mutation_cols = train_df.columns[1:]  # 'SUBCLASS' 이후 모든 열이 변이 정보\n",
    "train_df[mutation_cols] = train_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 4: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "# Step 5: 상관계수 기반으로 특징 선택 (상관계수가 낮은 특징 제거)\n",
    "correlation_matrix = abs(train_df.corr())\n",
    "correlation_with_target = correlation_matrix['SUBCLASS'].drop('SUBCLASS')\n",
    "top_features = correlation_with_target[correlation_with_target > 0.1].index\n",
    "\n",
    "# 피처 수를 줄임: 상위 1000개 또는 500개 피처만 사용\n",
    "top_n_features = 1000  # 조정 가능\n",
    "top_features = correlation_with_target.sort_values(ascending=False).head(top_n_features).index\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df['SUBCLASS']\n",
    "\n",
    "# Step 6: 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: XGBoost 모델 생성 및 하이퍼파라미터 튜닝\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: 모델 평가\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with reduced features: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 테스트 데이터에 대해 예측\n",
    "test_path = './input/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Step 9: 'ID' 컬럼 제거 후 테스트 데이터 전처리\n",
    "test_df = test_df.drop(columns=['ID'])\n",
    "test_df[mutation_cols] = test_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "test_df[mutation_cols] = scaler.transform(test_df[mutation_cols])\n",
    "\n",
    "# 테스트 데이터에서도 상위 중요한 피처만 선택\n",
    "X_test_final = test_df[top_features]\n",
    "\n",
    "# Step 10: Predict using the trained model\n",
    "y_test_pred = xgb_model.predict(X_test_final)\n",
    "\n",
    "# Step 11: Decode the predicted class labels back to their original subclass names\n",
    "y_test_pred_labels = le.inverse_transform(y_test_pred)\n",
    "\n",
    "# Step 12: Store the predictions in the test DataFrame\n",
    "test_df['Predicted_SUBCLASS'] = y_test_pred_labels\n",
    "\n",
    "# Display the first few rows of predictions\n",
    "print(test_df[['Predicted_SUBCLASS']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['Predicted_SUBCLASS']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['ID','Predicted_SUBCLASS']].to_csv('submit04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original test dataset to retrieve 'ID' column\n",
    "root_test = pd.read_csv(test_path)\n",
    "\n",
    "# Ensure 'ID' is preserved as a DataFrame\n",
    "id_column = root_test[['ID']]\n",
    "\n",
    "# Concatenate the 'ID' column with the predicted results\n",
    "final_result = pd.concat([id_column, test_df[['Predicted_SUBCLASS']]], axis=1)\n",
    "\n",
    "# Display the first few rows of the final result\n",
    "print(final_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result[['ID','Predicted_SUBCLASS']].to_csv('submit04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: 'ID' 컬럼 제거 (학습에 불필요한 열)\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "\n",
    "# Step 2: Encode the 'SUBCLASS' column (target labels)\n",
    "le = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 3: Convert mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "mutation_cols = train_df.columns[1:]  # 'SUBCLASS' 이후 모든 열이 변이 정보\n",
    "train_df[mutation_cols] = train_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 4: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "# Step 5: 상관계수 기반으로 특징 선택 (상관계수가 낮은 특징 제거)\n",
    "correlation_matrix = abs(train_df.corr())\n",
    "correlation_with_target = correlation_matrix['SUBCLASS'].drop('SUBCLASS')\n",
    "top_n_features = 2000  # 조정 가능\n",
    "top_features = correlation_with_target.sort_values(ascending=False).head(top_n_features).index\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df['SUBCLASS']\n",
    "\n",
    "# Step 6: 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: LightGBM 모델 생성 및 학습\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: 모델 평가\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with LightGBM: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대해 예측\n",
    "test_path = './input/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Step 9: 'ID' 컬럼 제거 후 테스트 데이터 전처리\n",
    "test_df = test_df.drop(columns=['ID'])\n",
    "test_df[mutation_cols] = test_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "test_df[mutation_cols] = scaler.transform(test_df[mutation_cols])\n",
    "\n",
    "# 테스트 데이터에서도 상위 중요한 피처만 선택\n",
    "X_test_final = test_df[top_features]\n",
    "\n",
    "# Step 10: Predict using the trained model\n",
    "y_test_pred = lgb_model.predict(X_test_final)\n",
    "\n",
    "# Step 11: Decode the predicted class labels back to their original subclass names\n",
    "y_test_pred_labels = le.inverse_transform(y_test_pred)\n",
    "\n",
    "# Step 12: Store the predictions in the test DataFrame\n",
    "test_df['Predicted_SUBCLASS'] = y_test_pred_labels\n",
    "\n",
    "# Display the first few rows of predictions\n",
    "print(test_df[['Predicted_SUBCLASS']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original test dataset to retrieve 'ID' column\n",
    "root_test = pd.read_csv(test_path)\n",
    "\n",
    "# Ensure 'ID' is preserved as a DataFrame\n",
    "id_column = root_test[['ID']]\n",
    "\n",
    "# Concatenate the 'ID' column with the predicted results\n",
    "final_result = pd.concat([id_column, test_df[['Predicted_SUBCLASS']]], axis=1)\n",
    "\n",
    "# Display the first few rows of the final result\n",
    "print(final_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result[['ID','Predicted_SUBCLASS']].to_csv('submit04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result['Predicted_SUBCLASS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: 'ID' 컬럼 제거 (학습에 불필요한 열)\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "\n",
    "# Step 2: Encode the 'SUBCLASS' column (target labels)\n",
    "le = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 3: Convert mutation columns from 'WT' and mutation strings to binary (0 for WT, 1 for mutation)\n",
    "mutation_cols = train_df.columns[1:]  # 'SUBCLASS' 이후 모든 열이 변이 정보\n",
    "train_df[mutation_cols] = train_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "\n",
    "# Step 4: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "\n",
    "# Step 5: 상관계수 기반으로 특징 선택 (상관계수가 낮은 특징 제거)\n",
    "correlation_matrix = abs(train_df.corr())\n",
    "correlation_with_target = correlation_matrix['SUBCLASS'].drop('SUBCLASS')\n",
    "top_n_features = 2000  # 조정 가능\n",
    "top_features = correlation_with_target.sort_values(ascending=False).head(top_n_features).index\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df['SUBCLASS']\n",
    "\n",
    "# Step 6: 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: 하이퍼파라미터 튜닝을 위한 GridSearchCV 설정\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 학습\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Step 8: 최적의 하이퍼파라미터로 학습된 모델 평가\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: 테스트 데이터에 대한 예측\n",
    "test_path = './input/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Step 10: 'ID' 컬럼 제거 후 테스트 데이터 전처리\n",
    "test_df = test_df.drop(columns=['ID'])\n",
    "test_df[mutation_cols] = test_df[mutation_cols].applymap(lambda x: 0 if x == 'WT' else 1)\n",
    "test_df[mutation_cols] = scaler.transform(test_df[mutation_cols])\n",
    "\n",
    "# 테스트 데이터에서도 상위 중요한 피처만 선택\n",
    "X_test_final = test_df[top_features]\n",
    "\n",
    "# Step 11: Predict using the best model\n",
    "y_test_pred = best_xgb.predict(X_test_final)\n",
    "\n",
    "# Step 12: Decode the predicted class labels back to their original subclass names\n",
    "y_test_pred_labels = le.inverse_transform(y_test_pred)\n",
    "\n",
    "# Step 13: Store the predictions in the test DataFrame\n",
    "test_df['Predicted_SUBCLASS'] = y_test_pred_labels\n",
    "\n",
    "# Display the first few rows of predictions\n",
    "print(test_df[['Predicted_SUBCLASS']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 729 candidates, totalling 2187 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s/.pyenv/versions/3.10.10/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Accuracy: 0.1200644641418211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.00      0.00      0.00        27\n",
      "           2       0.12      1.00      0.21       149\n",
      "           3       0.00      0.00      0.00        31\n",
      "           4       0.00      0.00      0.00        30\n",
      "           5       0.00      0.00      0.00        10\n",
      "           6       0.00      0.00      0.00       116\n",
      "           7       0.00      0.00      0.00        51\n",
      "           8       0.00      0.00      0.00        86\n",
      "           9       0.00      0.00      0.00        54\n",
      "          10       0.00      0.00      0.00        39\n",
      "          11       0.00      0.00      0.00        49\n",
      "          12       0.00      0.00      0.00        34\n",
      "          13       0.00      0.00      0.00        37\n",
      "          14       0.00      0.00      0.00        39\n",
      "          15       0.00      0.00      0.00        58\n",
      "          16       0.00      0.00      0.00        24\n",
      "          17       0.00      0.00      0.00        34\n",
      "          18       0.00      0.00      0.00        64\n",
      "          19       0.00      0.00      0.00        31\n",
      "          20       0.00      0.00      0.00        53\n",
      "          21       0.00      0.00      0.00        68\n",
      "          22       0.00      0.00      0.00        25\n",
      "          23       0.00      0.00      0.00        64\n",
      "          24       0.00      0.00      0.00        16\n",
      "          25       0.00      0.00      0.00        38\n",
      "\n",
      "    accuracy                           0.12      1241\n",
      "   macro avg       0.00      0.04      0.01      1241\n",
      "weighted avg       0.01      0.12      0.03      1241\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s/.pyenv/versions/3.10.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/s/.pyenv/versions/3.10.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/s/.pyenv/versions/3.10.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "train_sample_path = './input/train.csv'\n",
    "train_df = pd.read_csv(train_sample_path)\n",
    "\n",
    "# Step 1: 'ID' 컬럼 제거 (학습에 불필요한 열)\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "\n",
    "# Step 2: Encode the 'SUBCLASS' column (target labels)\n",
    "le_subclass = LabelEncoder()\n",
    "train_df['SUBCLASS'] = le_subclass.fit_transform(train_df['SUBCLASS'])\n",
    "\n",
    "# Step 3: 변이 값 처리 (WT는 0, 나머지는 LabelEncoder로 변환)\n",
    "mutation_cols = train_df.columns[1:]  # 'SUBCLASS' 이후 모든 열이 변이 정보\n",
    "le_mutations = LabelEncoder()\n",
    "\n",
    "# WT는 0으로 변환하고, 나머지는 LabelEncoder로 변환\n",
    "def encode_mutations(value):\n",
    "    if value == 'WT':\n",
    "        return 0\n",
    "    else:\n",
    "        return le_mutations.fit_transform([value])[0]\n",
    "\n",
    "for col in mutation_cols:\n",
    "    train_df[col] = train_df[col].apply(encode_mutations)\n",
    "\n",
    "# Step 4: Standardize the mutation features\n",
    "scaler = StandardScaler()\n",
    "train_df[mutation_cols] = scaler.fit_transform(train_df[mutation_cols])\n",
    "\n",
    "# Step 5: 상관계수 기반으로 특징 선택 (상관계수가 낮은 특징 제거)\n",
    "correlation_matrix = abs(train_df.corr())\n",
    "correlation_with_target = correlation_matrix['SUBCLASS'].drop('SUBCLASS')\n",
    "top_n_features = 2000  # 조정 가능\n",
    "top_features = correlation_with_target.sort_values(ascending=False).head(top_n_features).index\n",
    "\n",
    "X = train_df[top_features]\n",
    "y = train_df['SUBCLASS']\n",
    "\n",
    "# Step 6: 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: 하이퍼파라미터 튜닝을 위한 GridSearchCV 설정\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터로 학습\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Step 8: 최적의 하이퍼파라미터로 학습된 모델 평가\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Optimized Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Predicted_SUBCLASS\n",
      "0               BRCA\n",
      "1               BRCA\n",
      "2               BRCA\n",
      "3               BRCA\n",
      "4               BRCA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/x1jkjm3549s21y3h3k_k56q80000gn/T/ipykernel_53385/1267489115.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df['Predicted_SUBCLASS'] = y_test_pred_labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 9: 테스트 데이터에 대한 예측\n",
    "test_path = './input/test.csv'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Step 10: 'ID' 컬럼 제거 후 테스트 데이터 전처리\n",
    "test_df = test_df.drop(columns=['ID'])\n",
    "for col in mutation_cols:\n",
    "    test_df[col] = test_df[col].apply(encode_mutations)\n",
    "test_df[mutation_cols] = scaler.transform(test_df[mutation_cols])\n",
    "\n",
    "# 테스트 데이터에서도 상위 중요한 피처만 선택\n",
    "X_test_final = test_df[top_features]\n",
    "\n",
    "# Step 11: Predict using the best model\n",
    "y_test_pred = best_xgb.predict(X_test_final)\n",
    "\n",
    "# Step 12: Decode the predicted class labels back to their original subclass names\n",
    "y_test_pred_labels = le_subclass.inverse_transform(y_test_pred)\n",
    "\n",
    "# Step 13: Store the predictions in the test DataFrame\n",
    "test_df['Predicted_SUBCLASS'] = y_test_pred_labels\n",
    "\n",
    "# Display the first few rows of predictions\n",
    "print(test_df[['Predicted_SUBCLASS']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID Predicted_SUBCLASS\n",
      "0  TEST_0000               BRCA\n",
      "1  TEST_0001               BRCA\n",
      "2  TEST_0002               BRCA\n",
      "3  TEST_0003               BRCA\n",
      "4  TEST_0004               BRCA\n"
     ]
    }
   ],
   "source": [
    "# Load original test dataset to retrieve 'ID' column\n",
    "root_test = pd.read_csv(test_path)\n",
    "\n",
    "# Ensure 'ID' is preserved as a DataFrame\n",
    "id_column = root_test[['ID']]\n",
    "\n",
    "# Concatenate the 'ID' column with the predicted results\n",
    "final_result = pd.concat([id_column, test_df[['Predicted_SUBCLASS']]], axis=1)\n",
    "\n",
    "# Display the first few rows of the final result\n",
    "print(final_result.head())\n",
    "final_result[['ID','Predicted_SUBCLASS']].to_csv('submit04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicted_SUBCLASS\n",
       "BRCA                  2546\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[['Predicted_SUBCLASS']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
